{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ibm_boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a54f2f1e2101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mibm_boto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ibm_boto3'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('Reiniciar kernel después de insttalar Apache Spark')\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_9603a7b4cc7a4df9b1d5fda9f44d74f9 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='qcBdsvg-rBsPYy813yg--cYEyEG0u-umrkMSlRYglVoK',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_9603a7b4cc7a4df9b1d5fda9f44d74f9.get_object(Bucket='prueba1-donotdelete-pr-x3bdtqvtvyhuvc',Key='Canada.xlsx')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df_data_0 = pd.read_excel(body)\n",
    "df_data_0.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Agregar las siguientes columnas al dataframe: migración 80s, migración 90s, migracion 2000s, migración 2010s y total migración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_0[\"1980's\"]=df_data_0.iloc[:,7:17].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_0[\"1990's\"]=df_data_0.iloc[:,17:27].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_0[\"2000's\"]=df_data_0.iloc[:,27:41].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crear un nuevo df con la migración total por cada continente (solo 5 continentes) y convertirlo a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    " df=df_data_0.groupby(\"Continente\",as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Continente', '1980's', '1990's', '2000's'], dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[[\"Continente\", \"1980's\", \"1990's\",\"2000's\"]]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([ StructField(\"Continente\", StringType(), True)\\\n",
    "                       ,StructField(\"1980's\", IntegerType(), True)\\\n",
    "                       ,StructField(\"1990's\", IntegerType(), True)\\\n",
    "                       ,StructField(\"2000's\", IntegerType(), True)\\\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(df,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-------+\n",
      "|          Continente|1980's|1990's| 2000's|\n",
      "+--------------------+------+------+-------+\n",
      "|              Africa| 48815|148849| 421284|\n",
      "|                Asia|351025|938744|2028025|\n",
      "|              Europe|381738|548435| 480774|\n",
      "|Latin America and...|185975|242802| 336371|\n",
      "|    Northern America| 76824| 56931| 107387|\n",
      "|             Oceania| 13096| 18430|  23648|\n",
      "+--------------------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Crear un schema con los paises de oceania y las columnas de migración 80s, migración 90s, migracion 2000s, migración 2010s y total migración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_data_0[df_data_0[\"Continente\"]==\"Oceania\"][[\"Pais\", \"1980's\", \"1990's\",\"2000's\"]]\n",
    "df[\"Total_Migracion\"]=df.iloc[:,1::].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([ StructField(\"Pais\", StringType(), True)\\\n",
    "                       ,StructField(\"1980's\", IntegerType(), True)\\\n",
    "                       ,StructField(\"1990's\", IntegerType(), True)\\\n",
    "                       ,StructField(\"2000's\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Total_Migracion\", IntegerType(), True)\\\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(df,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+------+---------------+\n",
      "|            Pais|1980's|1990's|2000's|Total_Migracion|\n",
      "+----------------+------+------+------+---------------+\n",
      "|  American Samoa|     3|     2|     1|              6|\n",
      "|       Australia|  4564|  6574| 12691|          23829|\n",
      "|            Fiji|  5721|  9397|  5491|          20609|\n",
      "|        Kiribati|     3|     5|     7|             15|\n",
      "|Marshall Islands|     0|     0|     2|              2|\n",
      "|           Nauru|     2|     1|    15|             18|\n",
      "|   New Caledonia|     0|     0|     5|              5|\n",
      "|     New Zealand|  2719|  2344|  5323|          10386|\n",
      "|           Palau|     0|     0|     1|              1|\n",
      "|Papua New Guinea|    17|    16|    31|             64|\n",
      "|           Samoa|    23|    25|    30|             78|\n",
      "|           Tonga|    41|    60|    41|            142|\n",
      "|          Tuvalu|     3|     1|     4|              8|\n",
      "|         Vanuatu|     0|     5|     6|             11|\n",
      "+----------------+------+------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mostrar las estadisticas del Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|          Pais|            1980's|            1990's|           2000's|  Total_Migracion|\n",
      "+-------+--------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|            14|                14|                14|               14|               14|\n",
      "|   mean|          null| 935.4285714285714|1316.4285714285713|1689.142857142857|           3941.0|\n",
      "| stddev|          null|1935.5988027674346| 2945.115636081224|3716.966709090023|8241.599583351988|\n",
      "|    min|American Samoa|                 0|                 0|                1|                1|\n",
      "|    max|       Vanuatu|              5721|              9397|            12691|            23829|\n",
      "+-------+--------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mostar las columnas del Schema en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pais', \"1980's\", \"1990's\", \"2000's\", 'Total_Migracion']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Encontrar el promedio, la desviación estandard, el skewnesss y kurtosis de las columnas: migración 80s, migración 90s, migracion 2000s, migración 2010s y total migración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1=list(df[\"1980's\"])\n",
    "lista2=list(df[\"1990's\"])\n",
    "lista3=list(df[\"2000's\"])\n",
    "lista4=list(df['Total_Migracion'])\n",
    "listas=[lista1,lista2,lista3]\n",
    "lista_dec=[\"1980's\",\"1990's\",\"2000's\",'Total_Migracion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980's  PROMEDIO:  935.4285714285714\n",
      "1980's  STD:  1865.1896768458282\n",
      "1980's  skewness:  1.6982694823341717\n",
      "1980's  kurtosis:  4.226227843545893\n",
      "1990's  PROMEDIO:  1316.4285714285713\n",
      "1990's  STD:  2837.9844385530705\n",
      "1990's  skewness:  2.029732501977548\n",
      "1990's  kurtosis:  5.5724583580016915\n",
      "2000's  PROMEDIO:  1689.142857142857\n",
      "2000's  STD:  3581.758743114554\n",
      "2000's  skewness:  2.14952699539004\n",
      "2000's  kurtosis:  6.562484291933218\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(listas, lista_dec):\n",
    "    rdd=sc.parallelize(i)\n",
    "    suma = rdd.sum()\n",
    "    promedio=suma/rdd.count()\n",
    "    print(j,\" PROMEDIO: \",promedio)\n",
    "    res1= rdd.map(lambda x:pow(x-promedio,2)).sum()/rdd.count()\n",
    "    std = sqrt(res1)\n",
    "    print(j,\" STD: \",std)\n",
    "    res1= rdd.map(lambda x:pow(x-promedio,2)).sum()/rdd.count()\n",
    "    n = float(rdd.count())\n",
    "    skewness = 1/n*rdd.map(lambda x : pow(x-promedio,3)/pow(std,3)).sum()\n",
    "    print(j,\" skewness: \",skewness)\n",
    "    kurtosis = 1/n*rdd.map(lambda x : pow(x-promedio,4)/pow(std,4)).sum()\n",
    "    print(j,\" kurtosis: \",kurtosis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Obtener los valores maximos de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980's:  5721\n",
      "1990's:  9397\n",
      "2000's:  12691\n"
     ]
    }
   ],
   "source": [
    "maximo1=df2.agg({\"1980's\": \"max\"}).collect()[0][0]\n",
    "maximo2=df2.agg({\"1990's\": \"max\"}).collect()[0][0]\n",
    "maximo3=df2.agg({\"2000's\": \"max\"}).collect()[0][0]\n",
    "print(\"1980's: \",maximo1)\n",
    "print(\"1990's: \",maximo2)\n",
    "print(\"2000's: \",maximo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Cambiar el nombre de los paises a mayusculas utilizando .replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMERICAN SAMOA',\n",
       " 'AUSTRALIA',\n",
       " 'FIJI',\n",
       " 'KIRIBATI',\n",
       " 'MARSHALL ISLANDS',\n",
       " 'NAURU',\n",
       " 'NEW CALEDONIA',\n",
       " 'NEW ZEALAND',\n",
       " 'PALAU',\n",
       " 'PAPUA NEW GUINEA',\n",
       " 'SAMOA',\n",
       " 'TONGA',\n",
       " 'TUVALU',\n",
       " 'VANUATU']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pais=df2.select('Pais').rdd\n",
    "pais=pais.map(lambda x: x[0].upper())#.take(20)\n",
    "pais.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Encontrar el pais con más migración en los 90s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+---------------+\n",
      "|Pais|1980's|1990's|2000's|Total_Migracion|\n",
      "+----+------+------+------+---------------+\n",
      "|Fiji|  5721|  9397|  5491|          20609|\n",
      "+----+------+------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maximo=df2.agg({\"1990's\": \"max\"}).collect()\n",
    "df2.filter(df2[\"1990's\"]==maximo[0][0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
